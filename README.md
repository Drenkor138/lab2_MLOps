**Лабораторная работа 2: Разработка и контейнеризация ML-сервиса в Triton Inference Server**

## Цель работы

- разворачивать Triton Inference Server локально в Docker;
- использовать Python backend Triton для выполнения модели машинного обучения;
- упаковывать модель и код в модельный репозиторий Triton;
- реализовывать обработку текстов и инференс внутри Python-модуля модели;
- писать клиентский код для обращения к Triton API;
- тестировать работоспособность ML-сервиса.
